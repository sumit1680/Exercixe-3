/*
spark-shell --master yarn \
  --conf spark.ui.port=12345 \
  --num-executors 6 \
  --executor-cores 2 \
  --executor-memory 2G
*/

// Solution using Core API
val crimeData = sc.textFile("/public/crime/csv")
val header = crimeData.first
val crimeDataWithoutHeader = crimeData.filter(criminalRecord => criminalRecord != header)

val crimeCountForResidence = sc.parallelize(crimeDataWithoutHeader.
  filter(rec => rec.split(",(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)", -1)(7) == "RESIDENCE").
  map(rec => (rec.split(",(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)", -1)(5), 1)).
  reduceByKey((total, value) => total + value).
  map(rec => (rec._2, rec._1)).
  sortByKey(false).
  take(3))
  
//We can use takeOrdered versus sortByKey

crimeCountForResidence.
  map(rec => (rec._2, rec._1)).
  toDF("crime_type", "crime_count").
  write.json("user/sumitsinha1680/solutions/solution03/RESIDENCE_AREA_CRIMINAL_TYPE_DATA")
