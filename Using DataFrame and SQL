/*
spark-shell --master yarn \
  --conf spark.ui.port=12345 \
  --num-executors 6 \
  --executor-cores 2 \
  --executor-memory 2G
*/

// Solution using Core API
val crimeData = sc.textFile("/public/crime/csv")
val header = crimeData.first
val crimeDataWithoutHeader = crimeData.filter(criminalRecord => criminalRecord != header)

val crimeDataWithoutHeaderDF = crimeDataWithoutHeader.
  map(rec => {
    val r = rec.split(",(?=(?:[^\"]*\"[^\"]*\")*[^\"]*$)", -1)
    (r(7), r(5))
  }).toDF("location_description", "crime_type")

crimeDataWithoutHeaderDF.registerTempTable("crime_data")

sqlContext.setConf("spark.sql.shuffle.partitions", "4")
sqlContext.sql("select * from (" +
                 "select crime_type, count(1) crime_count " +
                 "from crime_data " +
                 "where location_description = 'RESIDENCE' " +
                 "group by crime_type " +
                 "order by crime_count desc) q " +
               "limit 3").
  coalesce(1).
  save("/user/sumitsinha1680/solutions/solution03/RESIDENCE_AREA_CRIMINAL_TYPE_DATA", "json")
